{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJsaUHuok9YX",
        "outputId": "763f4450-18a3-4b9c-8399-c42e458e9bd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: twikit in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
            "Requirement already satisfied: httpx[socks] in /usr/local/lib/python3.12/dist-packages (from twikit) (0.28.1)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.12/dist-packages (from twikit) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from twikit) (4.13.5)\n",
            "Requirement already satisfied: pyotp in /usr/local/lib/python3.12/dist-packages (from twikit) (2.9.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from twikit) (5.4.0)\n",
            "Requirement already satisfied: webvtt-py in /usr/local/lib/python3.12/dist-packages (from twikit) (0.5.1)\n",
            "Requirement already satisfied: m3u8 in /usr/local/lib/python3.12/dist-packages (from twikit) (6.0.0)\n",
            "Requirement already satisfied: Js2Py-3.13 in /usr/local/lib/python3.12/dist-packages (from twikit) (0.74.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->twikit) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->twikit) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx[socks]->twikit) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx[socks]->twikit) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx[socks]->twikit) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx[socks]->twikit) (3.11)\n",
            "Requirement already satisfied: socksio==1.* in /usr/local/lib/python3.12/dist-packages (from httpx[socks]->twikit) (1.0.0)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx[socks]->twikit) (0.16.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.12/dist-packages (from Js2Py-3.13->twikit) (5.3.1)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from Js2Py-3.13->twikit) (1.17.0)\n",
            "Requirement already satisfied: pyjsparser>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from Js2Py-3.13->twikit) (2.7.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx[socks]->twikit) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install twikit"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Library dan token twitter `Twikit` mengharuskan user menginputkan token sehingga tidak bisa anonim**"
      ],
      "metadata": {
        "id": "EAAsMm31Pe5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "import random\n",
        "import math\n",
        "from twikit import Client, TooManyRequests\n",
        "\n",
        "nest_asyncio.apply()\n",
        "# SETUP CLIENT + COOKIE\n",
        "client = Client(\"en-US\")\n",
        "client.set_cookies({\n",
        "    \"auth_token\": \"d125d285011b59146b60cdcd6edd9c82dca055c9\",\n",
        "    \"ct0\": \"59e9745d43723d4cb98f25be06a03b19dc235dbb0f0410ca8b0b49f8231a184b2bc947fcc2fd77668b90979426c200556e89b9d9dbd0089e9676e867fd25a2fe82a0380ac635254a31df26275aa7854e\",\n",
        "    \"twid\": \"u%3D1645176567038091265\",\n",
        "    \"gt\": \"1989338289653047338\",\n",
        "    \"guest_id\": \"v1%3A175121650454954896\",\n",
        "    \"guest_id_ads\": \"v1%3A175121650454954896\",\n",
        "    \"guest_id_marketing\": \"v1%3A175121650454954896\",\n",
        "    \"kdt\": \"HZdPjfR2pqsvUhxsO5GasalB5x7wYSzJ8sBbSx85\",\n",
        "    \"lang\": \"en\",\n",
        "    \"personalization_id\": \"v1_JcinIxPjnVOOctgz7rKeiQ==\",\n",
        "    \"cf_bm\": \"sjuQ5SgqIGcWGuc4AGCyaC42n3AikDN522y1tic1cGI-1763135431.7418268-1.0.1.1-3bCnpzTDAGCtwt0vvIoOgF_Li.Jdb5N0EUXsxh9yF6D9Kr7o3ZWHdWOGMmMYdqgGcF5rTpDcqAfpWxucyTaZlbd98K41EE4y6jyCmrn4tZwRPkpnfZsJPbUDKIDjxIY5\",\n",
        "    \"cuid\": \"f6d3d51a5a0345ee9e273cb98b2c5068\",\n",
        "    \"att\": \"1-PToudQBU2NDJeJZeJ8k2vit0taZstJ225z7kN54g\",\n",
        "    \"external_referer\": \"padhuUp37zhD6%2F29CpQtyhGQCUl05AFo|0|8e8t2xd8A2w%3D\",\n",
        "    \"g_state\": \"{\\\"i_l\\\":0,\\\"i_ll\\\":1763130464264}\",\n",
        "    \"gt\": \"1989338289653047338\",\n",
        "})\n"
      ],
      "metadata": {
        "id": "b3O_3YAnm7vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Buat parameter untuk scrapping**\n",
        "## **saya memberi delay lebih lama agar request tetap berjalan dan tidak di anggap spam**"
      ],
      "metadata": {
        "id": "0YAwAOKTQCqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config\n",
        "BATCH_SIZE = 20\n",
        "TARGET_PER_KEYWORD = 500\n",
        "CHECKPOINT_EVERY = 50\n",
        "MIN_DELAY = 10\n",
        "MAX_DELAY = 20\n",
        "MAX_RETRIES = 6\n",
        "CSV_FILENAME = \"tweet_gojek_grab.csv\"\n",
        "\n",
        "def write_header_if_needed(filename):\n",
        "    try:\n",
        "        with open(filename, \"x\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\"keyword\", \"tweet\"])\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "def append_rows(filename, rows):\n",
        "    with open(filename, \"a\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(rows)"
      ],
      "metadata": {
        "id": "JMLq2poZPRZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scraping dengan pagination, backoff, dan checkpointing\n",
        "async def scrape_keyword(keyword, target=TARGET_PER_KEYWORD):\n",
        "    print(f\"Start scraping keyword: {keyword} (target {target})\")\n",
        "    results = []\n",
        "    cursor = None\n",
        "    consecutive_empty = 0\n",
        "    write_header_if_needed(CSV_FILENAME)\n",
        "\n",
        "    while len(results) < target:\n",
        "        attempt = 0\n",
        "        while True:\n",
        "            try:\n",
        "                response = await client.search_tweet(\n",
        "                    f\"{keyword} lang:id\",\n",
        "                    product=\"Latest\",\n",
        "                    count=BATCH_SIZE,\n",
        "                    cursor=cursor\n",
        "                )\n",
        "                tweets = response if response is not None else []\n",
        "                break\n",
        "\n",
        "            except TooManyRequests:\n",
        "                attempt += 1\n",
        "                backoff = min(60, 2 ** attempt + random.random() * 5)\n",
        "                print(f\"429 Rate limit. Backoff for {backoff:.1f}s (attempt {attempt})\")\n",
        "                await asyncio.sleep(backoff)\n",
        "                if attempt >= MAX_RETRIES:\n",
        "                    print(\"Max retries reached for 429. Exiting this keyword early.\")\n",
        "                    return results\n",
        "\n",
        "            except Exception as e:\n",
        "                attempt += 1\n",
        "                backoff = min(60, (2 ** attempt) + random.random() * 5)\n",
        "                print(f\"transient error: {e!r}. Backoff {backoff:.1f}s (attempt {attempt})\")\n",
        "                await asyncio.sleep(backoff)\n",
        "                if attempt >= MAX_RETRIES:\n",
        "                    print(\"Max retries reached for errors. Exiting this keyword early.\")\n",
        "                    return results\n",
        "\n",
        "        if not tweets:\n",
        "            consecutive_empty += 1\n",
        "            print(f\"ℹNo tweets returned in this batch (empty count={consecutive_empty}).\")\n",
        "            if consecutive_empty >= 4:\n",
        "                print(\"ℹToo many consecutive empty batches — stopping pagination for this keyword.\")\n",
        "                break\n",
        "            await asyncio.sleep(random.uniform(MIN_DELAY, MAX_DELAY))\n",
        "            continue\n",
        "        else:\n",
        "            consecutive_empty = 0\n",
        "\n",
        "        batch_texts = []\n",
        "        for t in tweets:\n",
        "            try:\n",
        "                text = t.text\n",
        "            except Exception:\n",
        "                text = str(t)\n",
        "            batch_texts.append(text)\n",
        "\n",
        "        rows = []\n",
        "        for txt in batch_texts:\n",
        "            results.append(txt)\n",
        "            rows.append([keyword.replace(\" lang:id\",\"\"), txt])\n",
        "\n",
        "        if rows:\n",
        "            append_rows(CSV_FILENAME, rows)\n",
        "\n",
        "\n",
        "        print(f\"  got {len(batch_texts)} tweets (total collected: {len(results)}/{target})\")\n",
        "\n",
        "        try:\n",
        "            next_cursor = getattr(tweets, \"next_cursor\", None)\n",
        "            if next_cursor is None and isinstance(response, dict):\n",
        "                next_cursor = response.get(\"next_cursor\") or response.get(\"cursor\")\n",
        "        except Exception:\n",
        "            next_cursor = None\n",
        "\n",
        "        if not next_cursor:\n",
        "            if len(results) >= target:\n",
        "                break\n",
        "            cursor = None\n",
        "            print(\"no next_cursor; will try a few more small batches before stopping.\")\n",
        "        else:\n",
        "            cursor = next_cursor\n",
        "        await asyncio.sleep(random.uniform(MIN_DELAY, MAX_DELAY))\n",
        "        if len(results) >= target:\n",
        "            break\n",
        "\n",
        "    print(f\"Finished {keyword}: collected {len(results)} tweets.\")\n",
        "    return results[:target]"
      ],
      "metadata": {
        "id": "cLm9MDmBPWF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# main driver\n",
        "async def main():\n",
        "    write_header_if_needed(CSV_FILENAME)\n",
        "    keywords = [\"gojek\", \"grab\"]\n",
        "\n",
        "    final = {}\n",
        "    for kw in keywords:\n",
        "        tweets = await scrape_keyword(kw, TARGET_PER_KEYWORD)\n",
        "        final[kw] = tweets\n",
        "        cooldown = random.uniform(5, 12)\n",
        "        print(f\"Cooldown {cooldown:.1f}s before next keyword.\")\n",
        "        await asyncio.sleep(cooldown)\n",
        "\n",
        "    print(\"All done. Summary:\")\n",
        "    for k, v in final.items():\n",
        "        print(f\" - {k}: {len(v)} tweets collected\")\n",
        "await main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNmc91WXPZ-6",
        "outputId": "9e25e08c-893e-47a4-ea31-7e1e86d95de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start scraping keyword: gojek (target 500)\n",
            "  got 19 tweets (total collected: 19/500)\n",
            "  got 20 tweets (total collected: 39/500)\n",
            "  got 20 tweets (total collected: 59/500)\n",
            "  got 20 tweets (total collected: 79/500)\n",
            "  got 20 tweets (total collected: 99/500)\n",
            "  got 20 tweets (total collected: 119/500)\n",
            "  got 20 tweets (total collected: 139/500)\n",
            "  got 20 tweets (total collected: 159/500)\n",
            "  got 20 tweets (total collected: 179/500)\n",
            "  got 20 tweets (total collected: 199/500)\n",
            "  got 20 tweets (total collected: 219/500)\n",
            "  got 20 tweets (total collected: 239/500)\n",
            "  got 20 tweets (total collected: 259/500)\n",
            "  got 20 tweets (total collected: 279/500)\n",
            "  got 20 tweets (total collected: 299/500)\n",
            "  got 20 tweets (total collected: 319/500)\n",
            "  got 20 tweets (total collected: 339/500)\n",
            "  got 20 tweets (total collected: 359/500)\n",
            "  got 20 tweets (total collected: 379/500)\n",
            "  got 19 tweets (total collected: 398/500)\n",
            "  got 20 tweets (total collected: 418/500)\n",
            "  got 20 tweets (total collected: 438/500)\n",
            "  got 20 tweets (total collected: 458/500)\n",
            "  got 20 tweets (total collected: 478/500)\n",
            "  got 20 tweets (total collected: 498/500)\n",
            "  got 17 tweets (total collected: 515/500)\n",
            "Finished gojek: collected 515 tweets.\n",
            "Cooldown 5.1s before next keyword.\n",
            "Start scraping keyword: grab (target 500)\n",
            "  got 20 tweets (total collected: 20/500)\n",
            "  got 20 tweets (total collected: 40/500)\n",
            "  got 20 tweets (total collected: 60/500)\n",
            "  got 20 tweets (total collected: 80/500)\n",
            "  got 20 tweets (total collected: 100/500)\n",
            "  got 20 tweets (total collected: 120/500)\n",
            "  got 20 tweets (total collected: 140/500)\n",
            "  got 20 tweets (total collected: 160/500)\n",
            "  got 20 tweets (total collected: 180/500)\n",
            "  got 20 tweets (total collected: 200/500)\n",
            "  got 20 tweets (total collected: 220/500)\n",
            "  got 20 tweets (total collected: 240/500)\n",
            "  got 18 tweets (total collected: 258/500)\n",
            "  got 20 tweets (total collected: 278/500)\n",
            "  got 20 tweets (total collected: 298/500)\n",
            "  got 20 tweets (total collected: 318/500)\n",
            "  got 20 tweets (total collected: 338/500)\n",
            "  got 20 tweets (total collected: 358/500)\n",
            "  got 20 tweets (total collected: 378/500)\n",
            "  got 20 tweets (total collected: 398/500)\n",
            "  got 20 tweets (total collected: 418/500)\n",
            "  got 20 tweets (total collected: 438/500)\n",
            "  got 20 tweets (total collected: 458/500)\n",
            "  got 20 tweets (total collected: 478/500)\n",
            "  got 20 tweets (total collected: 498/500)\n",
            "  got 20 tweets (total collected: 518/500)\n",
            "Finished grab: collected 518 tweets.\n",
            "Cooldown 10.0s before next keyword.\n",
            "All done. Summary:\n",
            " - gojek: 500 tweets collected\n",
            " - grab: 500 tweets collected\n"
          ]
        }
      ]
    }
  ]
}